# -*- coding: utf-8 -*-
"""LVADSUSR120-SNEHAO-labq1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rUKfRpYPa4i8FLkWyDlJL_GwlrJHU4Ue
"""



import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from scipy import stats

# Load the dataset
df = pd.read_csv('/content/Fare prediction.csv')

# Handle missing values by imputing
imputer = SimpleImputer(strategy='mean')
df.fillna(df.mean(), inplace=True)

# Handle outliers (for demonstration, let's use Z-score method)
numerical_cols = df.select_dtypes(include=[np.number]).columns
z_scores = np.abs(stats.zscore(df[numerical_cols]))
threshold = 3
df_clean = df[(z_scores < threshold).all(axis=1)]

# EDA
print("Summary Statistics:")
print(df_clean.describe())

print("\nInfo:")
print(df_clean.info())

# Visualize distributions of numerical features
plt.figure(figsize=(12, 6))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(2, 4, i)
    sns.histplot(df_clean[col], kde=True)
    plt.title(col)
plt.tight_layout()
plt.show()

# Feature selection (for demonstration, let's use correlation method)
correlation_matrix = df_clean.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

selected_features = correlation_matrix['fare_amount'].sort_values(ascending=False).head(6).index.tolist()

# Normalize the data
scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df_clean[selected_features])
df_normalized = pd.DataFrame(df_normalized, columns=selected_features)

# Split the data into train and test sets
X = df_normalized.drop(columns=['fare_amount'])  # Features
y = df_normalized['fare_amount']  # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Visualize evaluation metrics
print("\nEvaluation Metrics:")
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("R^2 Score:", r2)

# Plot predictions vs. actual values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Fare Amount")
plt.ylabel("Predicted Fare Amount")
plt.title("Predictions vs. Actual Values")
plt.show()

"""Insights :The linear regression model developed for predicting fare amounts based on various features provides valuable insights into factors influencing taxi fares. Through analysis of feature importance and correlation, it's evident that variables like distance traveled, pickup time, and passenger count significantly affect fare amounts. Additionally, the model's evaluation metrics indicate its effectiveness in predicting fares accurately. This information can be utilized by taxi companies for optimizing pricing strategies, understanding customer preferences, and improving overall service efficiency.

"""